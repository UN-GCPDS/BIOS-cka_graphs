# -*- coding: utf-8 -*-
"""wpli-clusregreciones-cka.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19HU7xQvWHJkMm8iyxTKL5Ph3rf6mcG-t
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict, ShuffleSplit, StratifiedShuffleSplit, StratifiedKFold, KFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_absolute_error as mae
import tensorflow as tf
from scipy.stats import linregress
from scipy.stats import spearmanr, pearsonr
from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, RegressorMixin
from sklearn.model_selection import LeaveOneOut
import pickle
from joblib import load
from scipy.io import loadmat
from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer
from sklearn.metrics import pairwise_distances
from scipy.spatial.distance import squareform

from sklearn.manifold import TSNE
from sklearn.decomposition import PCA

from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin
from sklearn.model_selection import StratifiedShuffleSplit
import tensorflow_probability as tfp


class Keras_CKA(BaseEstimator, TransformerMixin):
  def __init__(self, epochs=30, batch_size=64, Q=0.9, learning_rate=1e-3, optimizer='Adam',
               l1_param=1e-3, validation_split=0, verbose=1, scale=1e-13):
        self.epochs = epochs
        self.scale = scale
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.l1_param = l1_param
        self.l2_param = l1_param
        self.validation_split = validation_split
        self.verbose = verbose
        self.optimizer = optimizer
        self.Q = Q

  # Define custom loss
  def custom_loss(self):
    # @tf.function()  #decorador para operar sobre python, mas lento y poco efectivo en muchos casos
    # Create a loss function that adds the MSE loss to the mean of all squared activations of a specific layer
    def custom_cka_loss(y_true, y_pred):  # ytrue labels, ypred  = Xw
      #kernels###############################################
      y_true = tf.cast(y_true, dtype=tf.float32)
      y_pred = tf.cast(y_pred, dtype=tf.float32)
      scalar_kernel = tfp.math.psd_kernels.ExponentiatedQuadratic(
        amplitude=1, length_scale=1)
      scalar_kernely = tfp.math.psd_kernels.ExponentiatedQuadratic(
        amplitude=1, length_scale=self.scale)
      k = scalar_kernel.matrix(y_pred, y_pred)
      l = scalar_kernely.matrix(y_true, y_true)
      #centralizar#####################################################
      N = tf.shape(l)[0]
      N2 = tf.cast(tf.shape(l)[0], dtype=tf.float32)
      # matrix for centered kernel
      h = tf.eye(N) - (1.0 / N2) * tf.ones([N, 1]) * tf.ones([1, N])
      trkl = tf.linalg.trace(tf.matmul(tf.matmul(k, h), tf.matmul(l, h)))
      trkk = tf.linalg.trace(tf.matmul(tf.matmul(k, h), tf.matmul(k, h)))
      trll = tf.linalg.trace(tf.matmul(tf.matmul(l, h), tf.matmul(l, h)))
      # funcion de costo############################################3
      # negative cka cost function (minimizing) f \in [-1,0]
      f = -trkl / tf.sqrt(trkk * trll)
      return f
    # Return a function
    return custom_cka_loss

  def fit(self, X, y):
    if self.optimizer == "Adam":
        opt = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)
    elif self.optimizer == "SGD":
        opt = tf.keras.optimizers.SGD(learning_rate=self.learning_rate)
    else:
        opt = self.optimizer

    tf.keras.backend.clear_session()

    if self.Q < 1:
      red = PCA(n_components=self.Q)
      red.fit(X)
      self.Q = red.components_.shape[0]
    ###acomodar arquitectura de red######
    inputA = tf.keras.layers.Input(shape=(X.shape[1]), name='entradaA')
    outputC = tf.keras.layers.Dense(self.Q, activation="linear", use_bias=False,
                                    kernel_regularizer=tf.keras.regularizers.l1_l2(
                                      l1=self.l1_param, l2=self.l2_param),
                                    name='outC_cka')(inputA)  # salida rotacion Lineal con cka

    self.model = tf.keras.Model(inputs=[inputA], outputs=[outputC])
    self.model.compile(loss=self.custom_loss(),
                       optimizer=opt, metrics="accuracy")
    self.history = self.model.fit(x=X, y=y, epochs=self.epochs, batch_size=self.batch_size,  # 32, 64, 128, 256
                                  validation_split=self.validation_split, verbose=self.verbose)

  def transform(self, X, *_):
    return self.model.predict(X)

  def fit_transform(self, X, y):
      self.fit(X, y)
      return self.transform(X)

  def plot_history_acc_w(self):
      plt.plot(self.history.history['loss'], label='loss')
      plt.plot(self.history.history['val_loss'], label='val_loss')
      plt.legend()
      return


FILEID = "1qL_Ms3ekjKkPBGet4H6m99txUwXmvk9h"
!wget - -load - cookies / tmp / cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILEID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id="$FILEID - O wpli_data.zip & & rm - rf / tmp / cookies.txt
!unzip 'wpli_data.zip'
#
dataWPLI = loadmat('./Grafos_rest_giga.mat')
acc = load(
  open("./Results_biclass_FBCSP_2tau_elastic_LDA_giga_acc (1).joblib", "rb"))

#
scoring = 'neg_mean_absolute_error'
dataWPLI = loadmat('./Grafos_rest_giga.mat')
acc = load(
  open("./Results_biclass_FBCSP_2tau_elastic_LDA_giga_acc (1).joblib", "rb"))
print(dataWPLI.keys())
print(acc.keys())

from sklearn.kernel_ridge import KernelRidge
from sklearn.model_selection import KFold
from scipy.stats import mode


def CKA_kernel_ridge(X, y):
  # CKA and Kernel Ridge
  steps = [('yscaler', StandardScaler()),
           ('redcka', Keras_CKA(epochs=150, batch_size=64, Q=0.9,
                                learning_rate=1e-3, scale=0.05, validation_split=0, verbose=0)),
           ('yscaler2', StandardScaler()),
           ('KernelRidge', KernelRidge(kernel='rbf'))
           ]
  parameters = {'KernelRidge__gamma': [1e-4, 1e-3, 1e-2, 1e-1],
                'KernelRidge__alpha': [1e-3, 1e-2, 1e-1]
                }

  pipeline = Pipeline(steps)
  # nested cross val
  cv = 25  # LeaveOneOut()
  cvi = 15  # LeaveOneOut()

  kf = KFold(n_splits=cv, random_state=30, shuffle=True)
  ypred = np.zeros(len(y)).reshape(-1, 1)
  j = 1
  list_hyper = []

  list_model = []

  for train_index, test_index in kf.split(X):
      print(('fold: %d/%d') % (j, cv))
      model_gd = GridSearchCV(pipeline, parameters, scoring=scoring,
                              cv=cvi, n_jobs=1, verbose=0)  # inner loop
      model_gd.fit(X[train_index], y[train_index])
      list_hyper.append(model_gd.best_params_)
      list_model.append(model_gd.best_estimator_)
      # evaluate best model
      print(model_gd.best_params_)
      ypred[test_index] = model_gd.predict(X[test_index]).reshape(-1, 1)
      j += 1
  return ypred

  # cka elastic Net
from sklearn.linear_model import ElasticNet

def Cka_ElasticNet(X, y):

  steps = [('yscaler', StandardScaler()),
           ('redcka', Keras_CKA(epochs=150, batch_size=64, Q=0.9,
                                learning_rate=1e-3, scale=0.05, validation_split=0, verbose=0)),
           ('yscaler2', StandardScaler()),
           ('ElasticNet_reg', ElasticNet())
           ]
  parameters = {'ElasticNet_reg__l1_ratio': [1e-3, 0.1, 0.25, 0.5],
                'ElasticNet_reg__alpha': [1e-3, 1e-2, 1e-1, 1]
                }

  pipeline = Pipeline(steps)
  # nested cross val
  cv = 25  # LeaveOneOut()
  cvi = 15  # LeaveOneOut()

  kf = KFold(n_splits=cv, random_state=30, shuffle=True)
  ypred = np.zeros(len(y)).reshape(-1, 1)
  j = 1
  list_hyper = []

  list_model = []

  for train_index, test_index in kf.split(X):
      print(('fold: %d/%d') % (j, cv))
      model_gd = GridSearchCV(pipeline, parameters, scoring=scoring,
                              cv=cvi, n_jobs=1, verbose=0)  # inner loop
      model_gd.fit(X[train_index], y[train_index])
      list_hyper.append(model_gd.best_params_)
      list_model.append(model_gd.best_estimator_)
      # evaluate best model
      print(model_gd.best_params_)
      ypred[test_index] = model_gd.predict(X[test_index]).reshape(-1, 1)
      j += 1
  return ypred


# canales en orden de relevancia kernel ridge:
orderchannel = [30, 21, 81, 8, 61, 1, 90, 123, 44, 37, 63, 99, 42, 46, 13, 15, 102, 78,
                83, 26, 124, 43, 105, 74, 50, 53, 70, 0, 48, 101, 98, 108, 88, 125, 107, 59,
                122, 3, 56, 116, 55, 84, 95, 58, 36, 7, 121, 72, 87, 80, 40, 2, 77, 113,
                28, 45, 104, 11, 68, 18, 52, 31, 10, 64, 35, 92, 89, 16, 12, 93, 79, 126,
                20, 57, 69, 114, 118, 9, 82, 66, 33, 19, 67, 71, 75, 41, 60, 73, 96, 76,
                34, 5, 14, 117, 85, 106, 25, 119, 111, 51, 109, 22, 38, 54, 24, 6, 39, 65,
                115, 27, 100, 110, 127, 103, 94, 97, 49, 91, 17, 112, 23, 86, 29, 32, 4, 120,
                47, 62]

chf1 = []
chf2 = []
for i in range(128):
  if orderchannel[i] < 64:
    chf1.append(orderchannel[i])
  else:
    chf2.append(orderchannel[i])

X = dataWPLI['clust']
y = acc['acc'].reshape(-1, 1) / 100

for i in range(16 - 1):
  canales = chf1[0:(i + 1) * 2] + chf2[0:(i + 1) * 2]
  print('canales alpha: ', chf1[0:(i + 1) * 2])
  print('canales betha Low: ', chf2[0:(i + 1) * 2])
  X_c = X[:, canales]
  print(X_c.shape)
  ypred = CKA_kernel_ridge(X_c, y)
  pearson = pearsonr(y.reshape(-1), ypred.reshape(-1))
  mae_ = mae(y.reshape(-1), ypred.reshape(-1))
  print('ridgetrain: ', pearson, mae_)
  results = {"ypred": ypred, "pearson": pearson, "MAE": mae_}
  with open('/clus_saveridgetrain"+str(X_c.shape[1])+".p', 'wb') as f:
    pickle.dump(results, f)

# canales en orden de relevancia Elastic net:
orderchannel = [37, 63, 1, 2, 88, 101, 34, 26, 92, 60, 56, 90, 105, 118, 39, 40, 74, 51,
                20, 52, 102, 5, 15, 93, 114, 17, 124, 6, 76, 115, 62, 97, 59, 83, 100, 31,
                110, 95, 125, 7, 13, 9, 38, 117, 126, 89, 70, 22, 81, 10, 65, 54, 108, 58,
                78, 49, 71, 61, 11, 48, 86, 104, 25, 44, 122, 127, 0, 91, 18, 24, 4, 42,
                109, 72, 8, 123, 33, 21, 28, 84, 53, 12, 106, 111, 30, 79, 77, 41, 107, 75,
                29, 32, 57, 3, 113, 98, 73, 47, 43, 19, 103, 68, 120, 46, 121, 67, 23, 55,
                119, 80, 36, 87, 112, 96, 99, 45, 64, 82, 14, 16, 35, 69, 66, 94, 27, 85,
                116, 50]

chf1 = []
chf2 = []
for i in range(128):
  if orderchannel[i] < 64:
    chf1.append(orderchannel[i])
  else:
    chf2.append(orderchannel[i])

X = dataWPLI['clust']
y = acc['acc'].reshape(-1, 1) / 100

for i in range(16 - 1):
  canales = chf1[0:(i + 1) * 2] + chf2[0:(i + 1) * 2]
  print('canales alpha: ', chf1[0:(i + 1) * 2])
  print('canales betha Low: ', chf2[0:(i + 1) * 2])
  X_c = X[:, canales]
  print(X_c.shape)
  ypred = Cka_ElasticNet(X_c, y)
  pearson = pearsonr(y.reshape(-1), ypred.reshape(-1))
  mae_ = mae(y.reshape(-1), ypred.reshape(-1))
  results = {"ypred": ypred, "pearson": pearson, "MAE": mae_}
  print('ElasticNe: ', pearson, mae_)
  pickle.dump(results, open("/clus_saveElasticNe" +
                            str(X_c.shape[1]) + ".p", "wb"))
